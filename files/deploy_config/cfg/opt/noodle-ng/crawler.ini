# in this file you can setup the crawler
# The %(here)s variable will be replaced with the parent directory of this file

[main]
# pick the form for your database
# %(here) may include a ':' character on Windows environments; this can
# invalidate the URI when specifying a SQLite db via path name
# sqlalchemy.url=postgres://username:password@hostname:port/databasename 
# sqlalchemy.url=mysql://username:password@hostname:port/databasename
sqlalchemy.url = mysql://noodle:noodlemittomatensosse@134.93.48.207:3306/noodle
sqlalchemy.echo = false

# Set the number of worker process to spawn and use
processes=10

# Set debug=true to enable debugging mode in the crawler 
# (more verbose output, no multiprocessing)
debug=false

# the credentials are used to try to login into the share
# anonymous is a special case where no password needs to be provided
credentials=["anonymous", ""]


# now you can define locations which the crawler should use.
#[LAN]
# a range defines a set of IP adresses which should be crawled
# it can have the following format:
#   '127.0.0.1'                # single ip
#   '192.168/16'               # CIDR network block
#   ('10.0.0.1', '10.0.0.19')  # inclusive range
# more info can be found here: http://code.google.com/p/python-iptools
#range=127.0.0.1, 192.168.0.0/16
# furthermore you can add  more credentials
#credentials=["user", "somesecretpassword"]

[Wohnheim]
credentials=["Gast", "123Dabei"]
range=134.93.48.1-134.93.80.1,
